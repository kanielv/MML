{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a portion of the dataset with equal distribution \n",
    "# df_pos = df[df['Sentiment'] == 1]\n",
    "# df_neg = df[df['Sentiment'] == 0]\n",
    "\n",
    "# df_pos = df_pos.sample(n=500, replace=False, random_state=1)\n",
    "# df_neg = df_neg.sample(n=500, replace=False, random_state=1)\n",
    "\n",
    "# df_subset = pd.concat([df_neg, df_pos], axis=0)\n",
    "\n",
    "# # change to lowercase \n",
    "# df_subset['Text'] = df_subset['Text'].apply(str.lower)\n",
    "# # df_subset.head()\n",
    "\n",
    "# # remove digits \n",
    "# df_subset['Text'] = df_subset['Text'].str.replace('\\d+', '', regex=True)\n",
    "# df_subset.head()\n",
    "\n",
    "# # decontraction \n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'won\\'t', 'will not', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'can\\'t', 'can not', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'n\\'t', ' not', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'re', ' are', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'s', ' is', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'d', ' would', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'ll', ' will', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'t', ' not', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'ve', ' have', regex=True)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace(r'\\'m', ' am', regex=True)\n",
    "\n",
    "# # remove special characters (usernames)\n",
    "# df_subset['Text'] = df_subset['Text'].str.replace('[^a-z0-9<>]', ' ', regex=True)\n",
    "\n",
    "# # apply stemming \n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# df_subset['Text'] = df_subset['Text'].apply(lambda i: stemmer.stem(i))\n",
    "\n",
    "# df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_sentence = [wordpunct_tokenize(i) for i in df_subset['Text']]\n",
    "# word2count = {}    \n",
    "\n",
    "# for sentence in tokenized_sentence:\n",
    "#     for word in sentence:\n",
    "#         if word not in word2count.keys():\n",
    "#             word2count[word] = 1\n",
    "#         else:\n",
    "#             word2count[word] += 1\n",
    "\n",
    "# unique_words = list(word2count.keys())\n",
    "\n",
    "# bag_of_words = []\n",
    "\n",
    "# for sentence in tokenized_sentence:\n",
    "#     bag_vector = np.zeros(len(unique_words))\n",
    "#     for idx, word in enumerate(sentence):\n",
    "#         for idx2, unique in enumerate(unique_words):\n",
    "#             if word == unique: \n",
    "#                 bag_vector[idx2] += 1\n",
    "#     bag_of_words.append(bag_vector)\n",
    "\n",
    "# bag_of_words"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
